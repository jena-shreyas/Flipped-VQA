The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) CCconfig         4) imkl/2020.1.217    7) libfabric/1.10.1
  2) gentoo/2020      5) intel/2020.1.217   8) openmpi/4.0.3
  3) gcccore/.9.3.0   6) ucx/1.8.0          9) StdEnv/2020

Lmod is automatically replacing "intel/2020.1.217" with "gcc/9.3.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) openmpi/4.0.3


The following have been reloaded with a version change:
  1) python/3.8.10 => python/3.10.2

  0%|          | 0/5 [00:00<?, ?it/s][W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
 20%|██        | 1/5 [05:58<23:53, 358.32s/it] 40%|████      | 2/5 [11:54<17:51, 357.32s/it] 40%|████      | 2/5 [15:03<22:34, 451.64s/it]
Traceback (most recent call last):
  File "train.py", line 154, in <module>
    main(args)
  File "train.py", line 130, in main
    train_stats = train_one_epoch(model, data_loader_train, optimizer, epoch, loss_scaler, args=args)
  File "/scratch/jenas/BTP/Flipped-VQA/engine.py", line 28, in train_one_epoch
    vqa_loss, vaq_loss, qav_loss = model(data)
  File "/home/jenas/envs/flipvqa/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jenas/envs/flipvqa/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/jenas/envs/flipvqa/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/jenas/BTP/Flipped-VQA/llama/model.py", line 258, in forward
    qav_h = layer(qav_h, start_pos, freqs_cis, mask, adapter[i].half(), None)
  File "/home/jenas/envs/flipvqa/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/jenas/BTP/Flipped-VQA/llama/model.py", line 158, in forward
    out = h + self.feed_forward.forward(self.ffn_norm(h))
  File "/home/jenas/envs/flipvqa/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/jenas/BTP/Flipped-VQA/llama/model.py", line 40, in forward
    output = self._norm(x.float()).type_as(x)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.74 GiB total capacity; 29.63 GiB already allocated; 17.62 MiB free; 30.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 215929) of binary: /home/jenas/envs/flipvqa/bin/python
Traceback (most recent call last):
  File "/home/jenas/envs/flipvqa/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/jenas/envs/flipvqa/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/jenas/envs/flipvqa/lib/python3.8/site-packages/torch/distributed/run.py", line 719, in main
    run(args)
  File "/home/jenas/envs/flipvqa/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/home/jenas/envs/flipvqa/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/jenas/envs/flipvqa/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-12-04_00:43:32
  host      : cdr2522.int.cedar.computecanada.ca
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 215929)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
