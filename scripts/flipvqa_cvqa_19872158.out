| distributed init (rank 0): env://, gpu 0
[01:01:37.521435] job dir: /scratch/jenas/BTP/Flipped-VQA
[01:01:37.521787] Namespace(accum_iter=2,
adapter_layer=32,
adapter_len=10,
batch_size=2,
bias=3.5,
blr=0.09,
dataset='causalvidqa',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=5,
gpu=0,
llama_model_path='./pretrained/llama/',
local_rank=-1,
lr=None,
max_feats=10,
max_seq_len=256,
min_lr=0.0,
model='7B',
num_workers=0,
output_dir='./checkpoint/causalvidqa',
pin_mem=False,
qav=True,
rank=0,
resume='',
seed=0,
start_epoch=0,
sub=False,
tau=100.0,
vaq=True,
warmup_epochs=2,
weight_decay=0.14,
world_size=1)
[01:01:48.339066] Num train data: 1500
[01:01:49.778544] Num val data: 1237
[01:01:49.779087] Loading model ...
[01:01:49.821379] Using model: 7B
[01:01:49.823055] loading from pretrained/llama/7B/consolidated.00.pth
[01:03:29.269674] Loaded checkpoints
[01:03:44.573560] Loaded model
[01:03:44.573790] base lr: 9.00e-02
[01:03:44.573843] actual lr: 1.41e-03
[01:03:44.573893] accumulate grad iterations: 2
[01:03:44.573937] effective batch size: 4
[01:03:44.918880] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.00140625
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.00140625
    weight_decay: 0.14
)
[01:03:44.919236] Start training for 5 epochs
[01:03:46.386119] Epoch: [0]  [  0/750]  eta: 0:18:04  lr: 0.000000  loss: 6.4849 (6.4849)  vqa_loss: 2.2148 (2.2148)  vaq_loss: 1.9697 (1.9697)  qav_loss: 2.3013 (2.3013)  time: 1.4456  data: 0.1054  max mem: 26571
[01:06:28.051706] Epoch: [0]  [187/750]  eta: 0:08:08  lr: 0.000174  loss: 3.5507 (4.8076)  vqa_loss: 0.5044 (1.3720)  vaq_loss: 0.8755 (1.2429)  qav_loss: 2.0602 (2.1926)  time: 0.8867  data: 0.0178  max mem: 26621
[01:09:11.234087] Epoch: [0]  [374/750]  eta: 0:05:27  lr: 0.000351  loss: 3.3802 (4.1210)  vqa_loss: 0.3489 (0.8937)  vaq_loss: 1.0752 (1.1482)  qav_loss: 1.9392 (2.0791)  time: 0.8653  data: 0.0134  max mem: 26621
[01:11:52.758795] Epoch: [0]  [561/750]  eta: 0:02:44  lr: 0.000525  loss: 2.6581 (3.7806)  vqa_loss: 0.3945 (0.7235)  vaq_loss: 0.5864 (1.0682)  qav_loss: 1.7778 (1.9889)  time: 0.8812  data: 0.0162  max mem: 26621
